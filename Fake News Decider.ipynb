{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Use NLP in sklearn to build a model which classifies news articles as being either fake or real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              title  \\\n",
      "0        8476                       You Can Smell Hillary’s Fear   \n",
      "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
      "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
      "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
      "4         875   The Battle of New York: Why This Primary Matters   \n",
      "\n",
      "                                                text label  \n",
      "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
      "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
      "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
      "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
      "4  It's primary day in New York and front-runners...  REAL  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6335 entries, 0 to 6334\n",
      "Data columns (total 4 columns):\n",
      "Unnamed: 0    6335 non-null int64\n",
      "title         6335 non-null object\n",
      "text          6335 non-null object\n",
      "label         6335 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 198.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('fake_or_real_news.csv')\n",
    "\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Unclear what purpose the first column serves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 3 0 ..., 0 0 0]\n",
      " [0 2 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Create a pandas series to store the labels\n",
    "y = df['label']\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'],y,test_size = 0.33,random_state = 53)\n",
    "# Why not use 'title' as well?\n",
    "\n",
    "# Initialize a CountVectorizer object: simply counts number of times a word appears\n",
    "count_vectorizer = CountVectorizer(stop_words = 'english')\n",
    "\n",
    "# Fit-Transform the training data into word vectors, ignoring stop words\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data into word vectors\n",
    "# Potential issue: words appearing in test data and not appearing in training data\n",
    "count_test = count_vectorizer.transform(X_test.values)\n",
    "\n",
    "print(type(count_train))\n",
    "print(count_train.A[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.02017005  0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.0332992   0.         ...,  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a TfidfVectorizer object: weight given by product of word frequency and log of inverse document frequency\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words = 'english', max_df=0.7)\n",
    "# max_df = 0.7 means \"ignore tokens that appear in more than 70% of the documents\" (in addition to stop words)\n",
    "\n",
    "# Fit-Transform the training data\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "# Transform the test data\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.values)\n",
    "\n",
    "# Print the first 25 vectors of the tfidf training data\n",
    "print(type(tfidf_train))\n",
    "print(tfidf_train.A[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.feature_extraction.text.CountVectorizer'> <class 'sklearn.feature_extraction.text.TfidfVectorizer'>\n",
      "   00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...    \\\n",
      "0   0    0     0         0       0      0     0       0      0      0  ...     \n",
      "1   0    0     0         0       0      0     0       0      0      0  ...     \n",
      "2   0    0     0         0       0      0     0       0      0      0  ...     \n",
      "3   0    0     0         0       0      0     0       0      0      0  ...     \n",
      "4   0    0     0         0       0      0     0       0      0      0  ...     \n",
      "\n",
      "   حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  \n",
      "0    0     0   0   0   0        0   0    0        0      0  \n",
      "1    0     0   0   0   0        0   0    0        0      0  \n",
      "2    0     0   0   0   0        0   0    0        0      0  \n",
      "3    0     0   0   0   0        0   0    0        0      0  \n",
      "4    0     0   0   0   0        0   0    0        0      0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "    00  000  0000  00000031  000035  00006  0001  0001pt  000ft  000km  ...    \\\n",
      "0  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...     \n",
      "1  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...     \n",
      "2  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...     \n",
      "3  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...     \n",
      "4  0.0  0.0   0.0       0.0     0.0    0.0   0.0     0.0    0.0    0.0  ...     \n",
      "\n",
      "   حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  \n",
      "0  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "1  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "2  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "3  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "4  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
      "\n",
      "[5 rows x 56922 columns]\n",
      "set()\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(type(count_vectorizer), type(tfidf_vectorizer))\n",
    "\n",
    "# Create the CountVectorizer DataFrame\n",
    "count_df = pd.DataFrame(data = count_train.A, columns = count_vectorizer.get_feature_names())\n",
    "\n",
    "# Create the TfidfVectorizer DataFrame\n",
    "tfidf_df = pd.DataFrame(data = tfidf_train.A, columns = tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "print(count_df.head())\n",
    "print(tfidf_df.head())\n",
    "\n",
    "# Calculate the difference between sets of columns\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "print(difference) # difference is the empty set\n",
    "\n",
    "# Check whether the DataFrames are equal\n",
    "print(count_df.equals(tfidf_df))\n",
    "# Same columns but different word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893352462936\n",
      "[[ 865  143]\n",
      " [  80 1003]]\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "# Instantiate a Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(count_train,y_train.values)\n",
    "\n",
    "# Create the predicted tags\n",
    "pred = nb_classifier.predict(count_test)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "accuracy = metrics.accuracy_score(y_test.values, pred)\n",
    "print(accuracy)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, pred, labels = ['FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This makes sense, since 865 + 143 + 80 + 1003 = 2091 = 0.33 x 6335, with 6335 total observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.856527977044\n",
      "[[ 739  269]\n",
      " [  31 1052]]\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags\n",
    "tfidf_pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculate the accuracy score\n",
    "tfidf_accuracy = metrics.accuracy_score(y_test, tfidf_pred)\n",
    "print(tfidf_accuracy)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "tfidf_cm = metrics.confusion_matrix(y_test, tfidf_pred, labels=['FAKE', 'REAL'])\n",
    "print(tfidf_cm)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A little less accurate.  MultinomialNB tends to be more conducive to (sparse) vectors with INTEGER entries.  Let's see if we can improve accuracy of the tfidf model with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.1\n",
      "Score:  0.897656623625\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.893830703013\n",
      "\n",
      "Alpha:  0.3\n",
      "Score:  0.890004782401\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.885700621712\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.884265901483\n",
      "\n",
      "Alpha:  0.6\n",
      "Score:  0.874701099952\n",
      "\n",
      "Alpha:  0.7\n",
      "Score:  0.870396939264\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.866092778575\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.858919177427\n",
      "\n",
      "Alpha:  1.0\n",
      "Score:  0.856527977044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the list of alpha values\n",
    "alphas = np.arange(0.1,1.1,0.1)\n",
    "# 1.0 is the default alpha value in MultinomialNB\n",
    "\n",
    "# Define train_and_predict()\n",
    "def train_and_predict(alpha):\n",
    "    # Instantiate the classifier\n",
    "    nb_classifier = MultinomialNB(alpha = alpha)\n",
    "    # Fit to the training data\n",
    "    nb_classifier.fit(tfidf_train, y_train)\n",
    "    # Predict the labels\n",
    "    pred = nb_classifier.predict(tfidf_test)\n",
    "    # Compute accuracy\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What are the most important features for a FAKE classification?  For a REAL classification?  Consider the tfidf model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL [('ראש', -11.316312804238807), ('רבה', -11.316312804238807), ('רלוונטיים', -11.316312804238807), ('רק', -11.316312804238807), ('שאוסלו', -11.316312804238807), ('שהוגדר', -11.316312804238807), ('שהיא', -11.316312804238807), ('שהיו', -11.316312804238807), ('שהמבצע', -11.316312804238807), ('שוך', -11.316312804238807), ('שולטים', -11.316312804238807), ('שזו', -11.316312804238807), ('שטחים', -11.316312804238807), ('שינוי', -11.316312804238807), ('שיתעקש', -11.316312804238807), ('שכל', -11.316312804238807), ('שכמוני', -11.316312804238807), ('של', -11.316312804238807), ('שלו', -11.316312804238807), ('שנדרש', -11.316312804238807), ('שני', -11.316312804238807), ('שעת', -11.316312804238807), ('שתי', -11.316312804238807), ('תאמצנה', -11.316312804238807), ('תוצאה', -11.316312804238807), ('תחל', -11.316312804238807), ('תיירות', -11.316312804238807), ('תנותק', -11.316312804238807), ('תעודת', -11.316312804238807), ('תתרכז', -11.316312804238807), ('أن', -11.316312804238807), ('إجلاء', -11.316312804238807), ('الأمر', -11.316312804238807), ('الجرحى', -11.316312804238807), ('الدولية', -11.316312804238807), ('القادمون', -11.316312804238807), ('اللجنة', -11.316312804238807), ('تحتاج', -11.316312804238807), ('تعرفه', -11.316312804238807), ('تنجح', -11.316312804238807), ('حلب', -11.316312804238807), ('عربي', -11.316312804238807), ('عن', -11.316312804238807), ('لم', -11.316312804238807), ('ما', -11.316312804238807), ('محاولات', -11.316312804238807), ('من', -11.316312804238807), ('هذا', -11.316312804238807), ('والمرضى', -11.316312804238807), ('ยงade', -11.316312804238807)]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels, namely FAKE and REAL\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Extract the features, as before\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# Zip the feature names together with the coefficient array and sort by weights (in what order?)\n",
    "feat_with_weights = sorted(zip(feature_names, nb_classifier.coef_[0]))\n",
    "\n",
    "# Top 50 features for FAKE classification (along with their weights)\n",
    "#print(class_labels[0], feat_with_weights[:50])\n",
    "\n",
    "# Top 50 features for REAL classification (equivalently, bottom 50 features for FAKE classification) \n",
    "print(class_labels[1], feat_with_weights[-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
